# æ·±åº¦å­¦ä¹  / Deep Learning

## ä¸»é¢˜ä»‹ç» / Topic Introduction

æ·±åº¦å­¦ä¹ æ˜¯ç°ä»£äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºã€‚æœ¬ä¸»é¢˜æ¶µç›–æ·±åº¦å­¦ä¹ çš„ç†è®ºåŸºç¡€ã€ç½‘ç»œæ¶æ„è®¾è®¡ã€ä¼˜åŒ–æ–¹æ³•ä»¥åŠå„ç§åº”ç”¨åœºæ™¯ã€‚æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ çš„åŸç†å¯¹äºæŒæ¡å½“ä»£AIæŠ€æœ¯è‡³å…³é‡è¦ã€‚

Deep learning is the core technology of modern artificial intelligence, using multi-layer neural networks to learn complex representations of data. This topic covers the theoretical foundations of deep learning, network architecture design, optimization methods, and various application scenarios. A deep understanding of deep learning principles is crucial for mastering contemporary AI technology.

## æ¨èå­¦ä¹ è·¯å¾„ / Recommended Learning Path

### ğŸ—ï¸ åŸºç¡€ç†è®º / Foundational Theory

1. çº¿æ€§ä»£æ•°ä¸æ¦‚ç‡è®ºåŸºç¡€ / Linear algebra and probability theory foundations
2. ç¥ç»ç½‘ç»œæ•°å­¦åŸç† / Mathematical principles of neural networks
3. åå‘ä¼ æ’­ç®—æ³• / Backpropagation algorithm

### ğŸ§  æ ¸å¿ƒæ¦‚å¿µ / Core Concepts

1. å„ç±»ç¥ç»ç½‘ç»œæ¶æ„ / Various neural network architectures
2. ä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ / Optimization algorithms and training techniques
3. æ­£åˆ™åŒ–ä¸æ³›åŒ– / Regularization and generalization

### ğŸš€ å‰æ²¿åº”ç”¨ / Advanced Applications

1. è®¡ç®—æœºè§†è§‰ / Computer vision
2. è‡ªç„¶è¯­è¨€å¤„ç† / Natural language processing
3. ç”Ÿæˆæ¨¡å‹ / Generative models

## ç²¾é€‰èµ„æ–™ / Curated Resources

### â­ ç»å…¸ç²¾é€‰ / Canonical Picks

- Deep Residual Learning for Image Recognition â€” ResNet é‡Œç¨‹ç¢‘è®ºæ–‡ã€‚/ ResNet milestone paper. [æŸ¥çœ‹ View](../_library/Deep_Residual_Learning_for_Image_Recognition.pdf)
- Batch Normalization â€” è®­ç»ƒç¨³å®šæ€§å’Œé€Ÿåº¦çš„é‡Œç¨‹ç¢‘ã€‚/ Landmark method for stable and fast training. [æŸ¥çœ‹ View](../_library/Batch_Normalization_Accelerating_Deep_Network_Training_by_Reducing_Internal_Covariate_Shift.pdf)
- Adam: A Method for Stochastic Optimization â€” éšæœºä¼˜åŒ–ç»å…¸ç®—æ³•ã€‚/ Classic optimizer. [æŸ¥çœ‹ View](../_library/Adam_A_Method_for_Stochastic_Optimization.pdf)

### ğŸ“š ç»å…¸æ•™æ / Classic Textbooks

 
#### Understanding Deep Learning Requires Rethinking Generalization

**ä½œè€…/Authors**: Chiyuan Zhang, Samy Bengio, Yoram Singer, et al.  
**å¹´ä»½/Year**: 2025  
**æ ‡ç­¾/Tags**: `æ³›åŒ–ç†è®º` `æ·±åº¦å­¦ä¹ ` `è¿‡æ‹Ÿåˆ` `Generalization` `Deep Learning` `Overfitting`

æœ¬è®ºæ–‡é€šè¿‡å®éªŒæ­ç¤ºæ·±åº¦ç½‘ç»œåœ¨è®­ç»ƒé›†æ‹Ÿåˆå…¨éƒ¨æ•°æ®çš„åŒæ—¶ä¾ç„¶åœ¨æµ‹è¯•é›†æ³›åŒ–è‰¯å¥½ï¼ŒæŒ‘æˆ˜äº†ç»å…¸çš„ç»Ÿè®¡å­¦ä¹ ç†è®ºã€‚è¿™ä¸€å‘ç°å¯¹ç†è§£æ·±åº¦å­¦ä¹ çš„æœ¬è´¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œæå‡ºäº†å…³äºæ·±åº¦ç½‘ç»œä¸ºä½•èƒ½åœ¨å®Œç¾æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ä»ä¿æŒå¼ºæ³›åŒ–èƒ½åŠ›çš„æ·±åˆ»é—®é¢˜ã€‚è®ºæ–‡ä¸ºæ·±åº¦å­¦ä¹ ç†è®ºç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

This work experimentally reveals that deep networks can generalize well on test sets even when they perfectly fit the training data, challenging classical statistical learning theories. This finding is significant for understanding the essence of deep learning and raises profound questions about why deep networks can maintain strong generalization while perfectly fitting training data.

**æ¨èè¯»è€…/Recommended For**: å¯¹æ·±åº¦ç½‘ç»œè®­ç»ƒè¡Œä¸ºæ„Ÿå…´è¶£çš„å­¦ä¹ è€…ï¼Œå°¤å…¶æ˜¯å¯¹"ä¸ºä½•èƒ½æ‹Ÿåˆå…¨éƒ¨æ•°æ®å´ä»æ³›åŒ–"é—®é¢˜æ„Ÿå…´è¶£çš„ç ”ç©¶è€…ã€‚/ For learners interested in the training behavior of deep networks, especially those puzzled by "perfect fitting yet strong generalization".

**é“¾æ¥/Link**: [../_library/Understanding_Deep_Learning_Requires_Rethinking_Generalization.pdf](../_library/Understanding_Deep_Learning_Requires_Rethinking_Generalization.pdf)

---

### ğŸ“„ æ›´å¤šèµ„æ–™ / More Resources

- Information Theory, Inference, and Learning Algorithms (MacKay) â€” ä¿¡æ¯è®ºä¸æ¨æ–­ç»å…¸æ•™æã€‚/ Classic information theory & inference text. [æŸ¥çœ‹ View](../_library/Information_Theory_Inference_And_Learning_Algorithms.pdf)
- Deep Residual Learning for Image Recognition â€” ResNet é‡Œç¨‹ç¢‘è®ºæ–‡ã€‚/ ResNet milestone paper. [æŸ¥çœ‹ View](../_library/Deep_Residual_Learning_for_Image_Recognition.pdf)
- Batch Normalization â€” è®­ç»ƒç¨³å®šæ€§å’Œé€Ÿåº¦çš„é‡è¦æ–¹æ³•ã€‚/ BN for stable and fast training. [æŸ¥çœ‹ View](../_library/Batch_Normalization_Accelerating_Deep_Network_Training_by_Reducing_Internal_Covariate_Shift.pdf)
- Layer Normalization â€” å˜é•¿åºåˆ—ä¸Šçš„å½’ä¸€åŒ–æ–¹æ³•ã€‚/ Normalization for sequences. [æŸ¥çœ‹ View](../_library/Layer_Normalization.pdf)
- Adam Optimizer â€” éšæœºä¼˜åŒ–é¢†åŸŸçš„ç»å…¸ç®—æ³•ã€‚/ Classic algorithm in stochastic optimization. [æŸ¥çœ‹ View](../_library/Adam_A_Method_for_Stochastic_Optimization.pdf)
- SGDR â€” ä½™å¼¦é€€ç«ä¸é‡å¯çš„æŠ€å·§ã€‚/ Cosine annealing with restarts. [æŸ¥çœ‹ View](../_library/SGDR_Stochastic_Gradient_Descent_with_Warm_Restarts.pdf)
- Software Engineering for Machine Learning â€” ML å·¥ç¨‹åŒ–æŒ‡å—ã€‚/ Engineering practices for ML. [æŸ¥çœ‹ View](../_library/Software_Engineering_for_Machine_Learning.pdf)



## ç›¸å…³ä¸»é¢˜ / Related Topics

- **è®­ç»ƒåŠ¨æ€ä¸æ³›åŒ–æœºåˆ¶** / Training Dynamics and Generalization â†’ [../06_Training_Dynamics_and_Generalization/](../06_Training_Dynamics_and_Generalization/)
- **AIåŸºç¡€ç†è®º** / AI Foundations â†’ [../04_AI_Foundations/](../04_AI_Foundations/)
- **NLP ä¸ Transformers** / NLP & Transformers â†’ [../05_Natural_Language_Processing_and_Transformers/](../05_Natural_Language_Processing_and_Transformers/)

## å­¦ä¹ å»ºè®® / Study Recommendations

### å¯¹äºåˆå­¦è€… / For Beginners

å»ºè®®ä»çº¿æ€§æ¨¡å‹å’ŒåŸºç¡€æ¦‚å¿µå¼€å§‹ï¼Œé€æ­¥ç†è§£ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ã€‚é‡ç‚¹å…³æ³¨æ•°å­¦åŸºç¡€çš„å»ºç«‹ã€‚

Start with linear models and basic concepts, gradually understanding how neural networks work. Focus on building mathematical foundations.

### å¯¹äºè¿›é˜¶å­¦ä¹ è€… / For Advanced Learners

æ·±å…¥ç ”ç©¶å„ç§ç½‘ç»œæ¶æ„çš„è®¾è®¡åŸç†ï¼Œç†è§£ä¸åŒä¼˜åŒ–æ–¹æ³•çš„é€‚ç”¨åœºæ™¯ï¼Œå¹¶é€šè¿‡å®è·µé¡¹ç›®åŠ æ·±ç†è§£ã€‚ç‰¹åˆ«å…³æ³¨æ³›åŒ–ç†è®ºçš„æœ€æ–°å‘å±•ã€‚

Study the design principles of various network architectures in depth, understand the applicable scenarios of different optimization methods, and deepen understanding through practical projects. Pay special attention to the latest developments in generalization theory.

---

æœ€åæ›´æ–° / Last Updated: 2025å¹´9æœˆ23æ—¥ / September 23, 2025
