# LLM ä¸ Transformers / LLMs and Transformers

## ç²¾é€‰èµ„æ–™ / Curated Resources

### ğŸ“„ æ›´å¤šèµ„æ–™ / More Resources

- Attention Is All You Need â€” Transformer å¼€å±±ä¹‹ä½œã€‚/ Foundational Transformer paper. [æŸ¥çœ‹ View](../_library/Attention%20Is%20All%20You%20Need.pdf)
- FlashAttention â€” é«˜æ•ˆç²¾ç¡®æ³¨æ„åŠ›å®ç°ã€‚/ Fast exact attention. [æŸ¥çœ‹ View](../_library/FlashAttention%20Fast%20and%20Memory-Efficient%20Exact%20Attention%20with%20IO-Awareness.pdf)
- LoRA: Low-Rank Adaptation â€” å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚/ Parameter-efficient finetuning. [æŸ¥çœ‹ View](../_library/LoRA%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models.pdf)
- Retrieval-Augmented Generation (RAG) â€” æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚/ RAG overview. [æŸ¥çœ‹ View](../_library/Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks.pdf)
- Direct Preference Optimization (DPO) â€” ç›´æ¥åå¥½ä¼˜åŒ–ã€‚/ DPO paper. [æŸ¥çœ‹ View](../_library/Direct%20Preference%20Optimization%20Your%20Language%20Model%20is%20Secretly%20a%20Reward%20Model.pdf)
- Learning to summarize from human feedback â€” äººç±»åé¦ˆå­¦ä¹ æ‘˜è¦ã€‚/ Summarization via RLHF. [æŸ¥çœ‹ View](../_library/Learning%20to%20summarize%20from%20human%20feedback.pdf)
- Training LMs to follow instructions with human feedback â€” æŒ‡ä»¤å¾®è°ƒã€‚/ Instruction tuning with human feedback. [æŸ¥çœ‹ View](../_library/Training%20language%20models%20to%20follow%20instructions%20with%20human%20feedback.pdf)
- Scaling Instruction-Finetuned LMs â€” æŒ‡ä»¤å¾®è°ƒè§„æ¨¡åŒ–ã€‚/ Scaling instruction tuning. [æŸ¥çœ‹ View](../_library/Scaling%20Instruction-Finetuned%20Language%20Models.pdf)
- Language Models are Few-Shot Learners â€” GPT-3 è®ºæ–‡ã€‚/ GPT-3. [æŸ¥çœ‹ View](../_library/Language%20Models%20are%20Few-Shot%20Learners.pdf)
- Large Language Models: A Survey â€” LLMç»¼è¿°ã€‚/ LLM survey. [æŸ¥çœ‹ View](../_library/Large%20Language%20Models%20A%20Survey.pdf)
- Harnessing the Power of LLMs in Practice â€” ChatGPT ç­‰å®è·µç»¼è¿°ã€‚/ Practical survey on ChatGPT and beyond. [æŸ¥çœ‹ View](../_library/Harnessing%20the%20Power%20of%20LLMs%20in%20Practice%20A%20Survey%20on%20ChatGPT%20and%20Beyond.pdf)
- Large Language Models Can Self-Improve â€” è‡ªæˆ‘æ”¹å–„ã€‚/ Self-improvement in LLMs. [æŸ¥çœ‹ View](../_library/Large%20Language%20Models%20Can%20Self-Improve.pdf)
- Self-Rewarding Language Models â€” è‡ªå¥–åŠ±è¯­è¨€æ¨¡å‹ã€‚/ Self-rewarding LMs. [æŸ¥çœ‹ View](../_library/Self-Rewarding%20Language%20Models.pdf)
- Efficient Training of LMs to Fill-in-the-Middle â€” FIM è®­ç»ƒã€‚/ FIM training. [æŸ¥çœ‹ View](../_library/Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle.pdf)

---

æœ€åæ›´æ–° / Last Updated: 2025-09-22
