# LLM 与 Transformers / LLMs and Transformers

## 精选资料 / Curated Resources

### ⭐ 经典精选 / Canonical Picks

- Attention Is All You Need — Transformer 开山之作。/ Foundational Transformer paper. [查看 View](../_library/Attention_Is_All_You_Need.pdf)
- Language Models are Few-Shot Learners — GPT‑3 里程碑论文。/ GPT‑3 milestone. [查看 View](../_library/Language_Models_are_Few-Shot_Learners.pdf)
- Training language models to follow instructions with human feedback — 指令微调与 RLHF 里程碑。/ Landmark instruction-following with human feedback. [查看 View](../_library/Training_language_models_to_follow_instructions_with_human_feedback.pdf)

### 📄 更多资料 / More Resources

- Attention Is All You Need — Transformer 开山之作。/ Foundational Transformer paper. [查看 View](../_library/Attention_Is_All_You_Need.pdf)
- FlashAttention — 高效精确注意力实现。/ Fast exact attention. [查看 View](../_library/FlashAttention_Fast_and_Memory-Efficient_Exact_Attention_with_IO-Awareness.pdf)
- LoRA: Low-Rank Adaptation — 参数高效微调。/ Parameter-efficient finetuning. [查看 View](../_library/LoRA_Low-Rank_Adaptation_of_Large_Language_Models.pdf)
- Retrieval-Augmented Generation (RAG) — 检索增强生成。/ RAG overview. [查看 View](../_library/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks.pdf)
- Direct Preference Optimization (DPO) — 直接偏好优化。/ DPO paper. [查看 View](../_library/Direct_Preference_Optimization_Your_Language_Model_is_Secretly_a_Reward_Model.pdf)
- Learning to summarize from human feedback — 人类反馈学习摘要。/ Summarization via RLHF. [查看 View](../_library/Learning_to_summarize_from_human_feedback.pdf)
- Training LMs to follow instructions with human feedback — 指令微调。/ Instruction tuning with human feedback. [查看 View](../_library/Training_language_models_to_follow_instructions_with_human_feedback.pdf)
- Scaling Instruction-Finetuned LMs — 指令微调规模化。/ Scaling instruction tuning. [查看 View](../_library/Scaling_Instruction-Finetuned_Language_Models.pdf)
- Language Models are Few-Shot Learners — GPT-3 论文。/ GPT-3. [查看 View](../_library/Language_Models_are_Few-Shot_Learners.pdf)
- Large Language Models: A Survey — LLM综述。/ LLM survey. [查看 View](../_library/Large_Language_Models_A_Survey.pdf)
- Harnessing the Power of LLMs in Practice — ChatGPT 等实践综述。/ Practical survey on ChatGPT and beyond. [查看 View](../_library/Harnessing_the_Power_of_LLMs_in_Practice_A_Survey_on_ChatGPT_and_Beyond.pdf)
- Large Language Models Can Self-Improve — 自我改善。/ Self-improvement in LLMs. [查看 View](../_library/Large_Language_Models_Can_Self-Improve.pdf)
- Self-Rewarding Language Models — 自奖励语言模型。/ Self-rewarding LMs. [查看 View](../_library/Self-Rewarding_Language_Models.pdf)
- Efficient Training of LMs to Fill-in-the-Middle — FIM 训练。/ FIM training. [查看 View](../_library/Efficient_Training_of_Language_Models_to_Fill_in_the_Middle.pdf)

---

最后更新 / Last Updated: 2025-09-22
